\section{Introduction}
\label{intro}

Consider the batch learning of a linear classifier with a differentiable convex loss function and an $L_2$ regularizer. This involves the minimization of a convex differentiable objective function $f(w)$ where $w$ is the weight vector. The minimization is usually performed using an iterative descent method in which an iteration starts from a point $w^r$, computes a direction $d^r$ that satisfies
\begin{equation}
\mbox{{\small\bf sufficient angle of descent:}}\; \phase{-g^r, d^r} \le \theta \label{angle}
\end{equation}
where $g^r=g(w^r)$, $g(w)=\grad f(w)$, $\phase{a,b}$ is the angle between vectors $a$ and $b$, and $0 \le \theta < \pi/2$, and then performs a line search along the direction $d^r$ to find the next point, $w^{r+1}=w^r+t d^r$. Let $w^\star=\arg\min_w f(w)$. A key contribution of this paper is the proof that, when $f$ is convex and satisfies some additional weak assumptions, the method has global linear rate of convergence ({\it glrc})\footnote{We say a method has {\it glrc} if $\exists$ $0<\delta<1$ such that $(f(w^{r+1})-f(w^\star)) \le \delta (f(w^r)-f(w^\star))\;\forall r$.} and so it finds a point $w^r$ satisfying $f(w^r)-f(w^\star)\le\epsilon$ in $O(log(1/\epsilon))$ iterations. {\it The main theme of this paper is that the flexibility offered by this method with good convergence properties allows us to build a class of useful distributed learning methods.}

Let us consider large scale learning in a distributed setting in which examples are partitioned over a number of computing nodes. Take one of the most effective distributed methods, viz., SQM (Statistical Query Model)~\cite{chu2006,agarwal2011}, which is a batch, gradient-based descent method. The gradient is computed in a distributed way with each node computing the gradient component corresponding to its set of examples. This is followed by an aggregation of the components. Consider a scenario in which the communication time between nodes is large relative to the computation time in each node.\footnote{This is the case when feature dimension is huge. Many applications gain performance when the feature space is expanded, say, via feature combinations, explicit expansion of nonlinear kernels etc.} In such a scenario, it is useful to ask: {\bf Q1.} {\it Can we do more computation in each node so that the number of communication passes is decreased, thus reducing the total computing time?}
%We will show that this question can be answered positively by using the iterative descent method suitably.

There have been some efforts in the literature to reduce the amount of communication. In these methods, the current $w^r$ is first passed on to all the nodes. Then, each node $p$ forms an approximation $\ftilde_p$ of $f$ using only its examples, followed by several optimization iterations (local passes over its examples) to decrease $\ftilde_p$ and reach a point $w_p$. The $w_p\;\forall p$ are averaged to form the next iterate $w^{r+1}$. One can stop after just one major iteration (going from $r=0$ to $r=1$); such a method is referred to as {\it parameter mixing (PM)}~\cite{mann2009}. Alternatively, one can do many major iterations; such a method is referred to as {\it iterative parameter mixing (IPM)}~\cite{hall2010}. Convergence theory for such methods is inadequate~\cite{mann2009, mcdonald2010}, which prompts us to ask: {\bf Q2.} {\it Is it possible to devise an IPM method that produces $\{w^r\} \rightarrow w^\star$?}

For large scale learning on a single machine, it is now well established that example-wise methods\footnote{These methods update $w$ after scanning each example.} such as stochastic gradient descent (SGD) and its variations~\cite{bottou2010, johnson2013} and dual coordinate ascent~\cite{hsieh2008} are much faster than batch gradient-based methods. However, example-wise methods are inherently sequential. If one employs a method such as SGD as the local optimizer for $\ftilde_p$ in PM/IPM, the result is, in essence, a parallel SGD method. However, convergence theory for such a method is limited, even that requiring a complicated analysis~\cite{zinkevich2010}. Thus, we ask: {\bf Q3.} {\it Can we form a parallel SGD method with strong convergence properties?}

We make a novel and simple use of the iterative descent method mentioned at the beginning of this section to design a distributed algorithm that answers Q1-Q3 positively. The main idea is to use distributed computation for generating a good search direction $d^r$ and not just for forming the gradient as in SQM. At iteration $r$, let us say each node $p$ has the current iterate $w^r$ and the gradient $g^r$. This information can be used together with the examples in the node to form a function $\fhat_p(\cdot)$ that approximates $f(\cdot)$ and satisfies $\grad\fhat_p(w^r)=g^r$. One simple and effective suggestion is:
\begin{equation}
\fhat_p(w)=f_p(w)+(g^r - \nabla f_p(w^r))\cdot(w-w^r)
\label{sugg1}
\end{equation}
where $f_p$ is the part of $f$ that does not depend on examples outside node $p$. In section~\ref{distr} we give other suggestions for forming $\fhat_p$. Now $\fhat_p$ can be optimized within node $p$ using any method ${\cal M}$ which has {\it glrc}, e.g., Trust region method, L-BFGS, etc. There is no need to optimize $\fhat_p$ fully. We show (see section~\ref{distr}) that, in a constant number of local passes over examples in node $p$, an approximate minimizer $w_p$ of $\fhat_p$ can be found such that the direction $d_p=w_p-w^r$ satisfies the sufficient angle of descent condition, (\ref{angle}). The set of directions generated in the nodes, $\{d_p\}$ can be averaged to form the overall direction $d^r$ for iteration $r$. Note that $d^r$ also satisfies (\ref{angle}). The result is an overall distributed method that finds a point $w$ satisfying $f(w)-f(w^\star)\le\epsilon$ in $O(\log (1/\epsilon))$ time. This answers {\bf Q2}.

The method also reduces the number of distributed passes over the examples compared with SQM, thus also answering {\bf Q1}. The intuition here is that, if each $\fhat_p$ is a good approximation of $f$, then $d^r$ will be a good global direction for minimizing $f$ at $w^r$, and so the method will move towards $w^\star$ much faster than SQM.
As one special instantiation of our distributed method, we can use, for the local optimization method ${\cal M}$, any variation of SGD with {\it glrc} (in expectation), e.g., the one in Johnson \& Zhang~\yrcite{johnson2013}. For this case, our method has $O(\log (1/\epsilon))$ time convergence in a probabilistic sense (Mahajan et al. NIPS workshop paper). The result is a strongly convergent parallel SGD method, which answers {\bf Q3}. An interesting side observation is that, the single machine version of this instantiation is very close to the variance-reducing SGD method in Johnson \& Zhang~\yrcite{johnson2013}.


%In section~\ref{expts} we demonstrate this via experiments on distributed implementations of the methods using {\it AllReduce} on Hadoop.

%The set up described above can also be used to to devise a method which fills an important gap in the distributed learning literature. Let us first describe this gap before describing our solution. For large scale learning on a single machine, it is now well established that example-wise methods\footnote{These methods update $w$ after scanning each example.} such as stochastic gradient descent (SGD) and its variations~\cite{bottou2010, johnson2013} and dual coordinate ascent~\cite{hsieh2008} are much faster than batch gradient-based methods. However, example-wise methods are inherently sequential. There have been attempts to parallelize them using the following idea: pass the current $w^r$ to all nodes, in each node $p$ apply one or more epochs of SGD on its examples to generate $w_p$, and then average the $w_p$ using a {\it Reduce} operation to form $w^{r+1}$. One can stop such {\it parameter mixing} methods after a single iteration~\cite{zinkevich2010, mann2009} (going from $r=0$ to $r=1$) or do several iterations~\cite{hall2010, mcdonald2010}; however, establishing convergence of such methods is limited as well as hard.

%Let us now briefly describe our solution. Suppose, in our distributed method described earlier, we form $\fhat_p$ using (\ref{sugg1}), use a SGD method as the choice for $\cal{M}$ to minimize $\fhat_p$, and run several epochs of SGD updates over the examples of node $p$, then a $w_p$ can be generated so that $d_p=w_p-w^r$ satisfies (\ref{angle}). Averaging the $d_p$ to generate $d^r$ followed by line search completes an iteration. The resulting instantiation of Algorithm~\ref{GD} has good convergence properties  and is a nice parallelization of SGD. The ability to inject an example-wise method in a batch method is interesting as well as a demonstration of the flexibility offered by our approach to large scale distributed learning. Another interesting observation is that, the single machine version of this instantiation is very close to a recently proposed variance-reducing SGD method~\cite{johnson2013}.

In summary, the paper makes the following contributions. (1) For convex $f$ we establish {\it glrc} for a general iterative descent method. (2) We propose a distributed learning algorithm that: (a) converges in $O(\log (1/\epsilon))$ time, thus leading to an IPM method with strong convergence; (b) is more efficient than SQM when communication times are high; and (c) flexible in terms of the local optimization method ${\cal M}$ that can be used in the nodes. (3) We give an effective parallelization of SGD with good theoretical support and make connections with a recently proposed variance-reducing SGD method. 

Experiments validate our theory as well as show the benefits of our method for large dimensional datasets where communication is the bottleneck. We conclude with a discussion on unexplored possibilities for extending our distributed learning method in section~\ref{disc}.

%We close this section with a brief discussion of works related to this paper.

%\noindent {\bf Related work.} Let us begin with papers related to the general descent method and its convergence.
