\begin{thebibliography}{22}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Agarwal et~al.(2011)Agarwal, Chapelle, Dudik, and
  Langford]{agarwal2011}
Agarwal, A., Chapelle, O., Dudik, M., and Langford, J.
\newblock A reliable effective terascale linear learning system.
\newblock In \emph{arXiv}, 2011.

\bibitem[Bottou(2010)]{bottou2010}
Bottou, L.
\newblock Large-scale machine learning with stochastic gradient descent.
\newblock In \emph{COMPSTAT'2010}, pp.\  177--187, 2010.

\bibitem[Boyd \& Vandenberghe(2004)Boyd and Vandenberghe]{boyd2004}
Boyd, S. and Vandenberghe, L.
\newblock \emph{Convex optimization}.
\newblock Cambridge University Press, Cambridge, UK, 2004.

\bibitem[Boyd et~al.(2011)Boyd, Parikh, Chu, Peleato, and Eckstein]{Boyd2011}
Boyd, S., Parikh, N., Chu, E., Peleato, B., and Eckstein, J.
\newblock Distributed optimization and statistical learning via the alternating
  direction method of multipliers.
\newblock \emph{Foundations and Trends in Machine Learning}, pp.\  1--122,
  2011.

\bibitem[Chang et~al.(2008)Chang, Hsieh, and Lin]{chang2008}
Chang, K.W., Hsieh, C.J., and Lin, C.J.
\newblock Coordinate descent method for large-scale l2-loss linear svm.
\newblock \emph{JMLR}, pp.\  1369--1398, 2008.

\bibitem[Chu et~al.(2006)Chu, Kim, Lin, Yu, Bradski, Ng, and Olukotun]{chu2006}
Chu, C.T., Kim, S.K., Lin, Y.A., Yu, Y.Y., Bradski, G., Ng, A.Y., and Olukotun,
  K.
\newblock Map-reduce for machine learning on multicore.
\newblock \emph{NIPS}, pp.\  281--288, 2006.

\bibitem[Hall et~al.(2010)Hall, Gilpin, and Mann]{hall2010}
Hall, K.B., Gilpin, S., and Mann, G.
\newblock Mapreduce/bigtable for distributed optimization.
\newblock In \emph{NIPS Workshop on Leaning on Cores, Clusters, and Clouds},
  2010.

\bibitem[Hsieh et~al.(2008)Hsieh, Chang, Lin, Keerthi, and
  Sundararajan]{hsieh2008}
Hsieh, C.J., Chang, K.W., Lin, C.J., Keerthi, S.S., and Sundararajan, S.
\newblock A dual coordinate descent method for large-scale linear svm.
\newblock In \emph{ICML}, pp.\  408--415, 2008.

\bibitem[Johnson \& Zhang(2013)Johnson and Zhang]{johnson2013}
Johnson, R. and Zhang, T.
\newblock Accelerating stochastic gradient descent using predictive variance
  reduction.
\newblock \emph{NIPS}, 2013.

\bibitem[Le~Roux et~al.(2012)Le~Roux, Schmidt, and Bach]{leroux2012}
Le~Roux, N., Schmidt, M., and Bach, F.
\newblock A stochastic gradient method with an exponential convergence rate for
  strongly convex optimization with finite training sets.
\newblock In \emph{arXiv}, 2012.

\bibitem[Lin et~al.(2008)Lin, Weng, and Keerthi]{lin2008}
Lin, C.J., Weng, R.C., and Keerthi, S.S.
\newblock Trust region newton method for large-scale logistic regression.
\newblock \emph{JMLR}, pp.\  627--650, 2008.

\bibitem[Liu \& Nocedal(1989)Liu and Nocedal]{liu89}
Liu, D.~C. and Nocedal, J.
\newblock On the limited memory {B}{F}{G}{S} method for large scale
  optimization.
\newblock \emph{Math. Programming}, 45\penalty0 (3, (Ser. B)):\penalty0
  503--528, 1989.

\bibitem[Mann et~al.(2009)Mann, McDonald, Mohri, Silberman, and
  Walker]{mann2009}
Mann, G., McDonald, R.T., Mohri, M., Silberman, N., and Walker, D.
\newblock Efficient large-scale distributed training of conditional maximum
  entropy models.
\newblock In \emph{NIPS}, pp.\  1231--1239, 2009.

\bibitem[McDonald et~al.(2010)McDonald, Hall, and Mann]{mcdonald2010}
McDonald, R.T., Hall, K., and Mann, G.
\newblock Distributed training strategies for the structured perceptron.
\newblock In \emph{HLT-NAACL}, pp.\  456--464, 2010.

\bibitem[Patriksson(1998{\natexlab{a}})]{patriksson1998ca}
Patriksson, M.
\newblock Cost approximation: A unified framework of descent algorithms for
  nonlinear programs.
\newblock \emph{SIAM J. on Optimization}, 8:\penalty0 561--582,
  1998{\natexlab{a}}.

\bibitem[Patriksson(1998{\natexlab{b}})]{patriksson1998fp}
Patriksson, M.
\newblock Decomposition methods for differentiable optimization problems
  overcartesian product sets.
\newblock \emph{Comput. Optim. Appl.}, 9:\penalty0 5--42, 1998{\natexlab{b}}.

\bibitem[Richt{\'a}rik \& Tak{\'a}c(2012)Richt{\'a}rik and
  Tak{\'a}c]{richtarik2013}
Richt{\'a}rik, P. and Tak{\'a}c, M.
\newblock Parallel coordinate descent methods for big data optimization.
\newblock \emph{CoRR}, abs/1212.0873, 2012.

\bibitem[Smola \& Vishwanathan(2008)Smola and Vishwanathan]{smola2008}
Smola, A. and Vishwanathan, S.V.N.
\newblock \emph{Introduction to Machine Learning}.
\newblock Cambridge University Press, Cambridge, UK, 2008.

\bibitem[Wang \& Lin(2013)Wang and Lin]{wang2013}
Wang, P.W. and Lin, C.J.
\newblock Iteration complexity of feasible descent methods for convex
  optimization.
\newblock \emph{Technical Report, National Taiwan University}, 2013.

\bibitem[Wolfe(1969)]{wolfe1969}
Wolfe, P.
\newblock Convergence conditions for ascent methods.
\newblock \emph{SIAM Review}, 11:\penalty0 226--235, 1969.

\bibitem[Wolfe(1971)]{wolfe1971}
Wolfe, P.
\newblock Convergence conditions for ascent methods: {II}: {S}ome corrections.
\newblock \emph{SIAM Review}, 13:\penalty0 185--188, 1971.

\bibitem[Zinkevich et~al.(2010)Zinkevich, Weimer, Smola, and Li]{zinkevich2010}
Zinkevich, M., Weimer, M., Smola, A., and Li, L.
\newblock Parallelized stochastic gradient descent.
\newblock In \emph{NIPS}, pp.\  2595--2603, 2010.

\end{thebibliography}
